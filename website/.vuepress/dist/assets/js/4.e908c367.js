(window.webpackJsonp=window.webpackJsonp||[]).push([[4],{184:function(e,t,a){e.exports=a.p+"assets/img/supervisedfeatureselectionmethods.ea13a693.png"},200:function(e,t,a){"use strict";a.r(t);var i=a(0),r=Object(i.a)({},function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[i("h1",{attrs:{id:"dimensionality-reduction-in-supervised-learning-feature-selection"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#dimensionality-reduction-in-supervised-learning-feature-selection","aria-hidden":"true"}},[e._v("#")]),e._v(" Dimensionality Reduction in Supervised Learning: Feature Selection")]),e._v(" "),i("p",[e._v("We should select the most and least important feature.")]),e._v(" "),i("div",{staticClass:"tip custom-block"},[i("p",[e._v("A process that chooses an optimal subset of features according to a objective function.")])]),e._v(" "),i("h2",{attrs:{id:"objectives"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#objectives","aria-hidden":"true"}},[e._v("#")]),e._v(" Objectives")]),e._v(" "),i("ul",[i("li",[e._v("To reduce Dimensionality and remove noise")]),e._v(" "),i("li",[e._v("To improve mining performance")])]),e._v(" "),i("h2",{attrs:{id:"feature-selection-methods"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#feature-selection-methods","aria-hidden":"true"}},[e._v("#")]),e._v(" Feature Selection Methods")]),e._v(" "),i("ul",[i("li",[e._v("Embedded")]),e._v(" "),i("li",[e._v("Filter")]),e._v(" "),i("li",[e._v("Wrapper")])]),e._v(" "),i("p",[i("img",{attrs:{src:a(184),alt:"featureselectionmethods"}})]),e._v(" "),i("h3",{attrs:{id:"embedded"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#embedded","aria-hidden":"true"}},[e._v("#")]),e._v(" Embedded")]),e._v(" "),i("div",{staticClass:"tip custom-block"},[i("p",[e._v("a machine learning algorithm that returns a model using a limited number of features")])]),e._v(" "),i("p",[e._v("They are a part of induction algorithms")]),e._v(" "),i("h3",{attrs:{id:"filter"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#filter","aria-hidden":"true"}},[e._v("#")]),e._v(" Filter")]),e._v(" "),i("p",[e._v("They are separate processes from the induction algorithms")]),e._v(" "),i("h3",{attrs:{id:"wrapper"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#wrapper","aria-hidden":"true"}},[e._v("#")]),e._v(" Wrapper")]),e._v(" "),i("p",[e._v("They are also separate processes from induction algorithm but they use induction algorithm as a subroutine")]),e._v(" "),i("h2",{attrs:{id:"regularization-techniques"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#regularization-techniques","aria-hidden":"true"}},[e._v("#")]),e._v(" Regularization Techniques")]),e._v(" "),i("p",[e._v("Penalizing the magnitude of coefficients of features along with minimizing the error between predicted and actual observations, in case of large number of features")]),e._v(" "),i("div",{staticClass:"tip custom-block"},[i("p",[e._v("Example: assign zero weight for features that are not very important makes a large difference in the target variable")])]),e._v(" "),i("h3",{attrs:{id:"why-use-regularization"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#why-use-regularization","aria-hidden":"true"}},[e._v("#")]),e._v(" Why Use Regularization?")]),e._v(" "),i("ul",[i("li",[e._v("To reduce model complexity")]),e._v(" "),i("li",[e._v("To avoid overfitting to the particular training data")]),e._v(" "),i("li",[e._v("Upgrade the predictability of the model")]),e._v(" "),i("li",[e._v("Finding the optimal weights")])]),e._v(" "),i("h3",{attrs:{id:"ridge-regression"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#ridge-regression","aria-hidden":"true"}},[e._v("#")]),e._v(" Ridge Regression")]),e._v(" "),i("h3",{attrs:{id:"lasso-regression"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#lasso-regression","aria-hidden":"true"}},[e._v("#")]),e._v(" Lasso Regression")])])},[],!1,null,null,null);t.default=r.exports}}]);